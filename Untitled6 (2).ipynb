{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd9c70-6681-4092-8ce2-d35d85534d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "Answer--The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a set of training data \n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " ), where \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  represents the feature vectors and \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  represents the class labels, the linear SVM aims to find the optimal hyperplane that separates the data into two classes.\n",
    "\n",
    "For a binary classification problem, the decision function of a linear SVM can be expressed as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "f(x)=w \n",
    "T\n",
    " x+b\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) is the decision function that determines the class label of input vector \n",
    "�\n",
    "x.\n",
    "�\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "�\n",
    "x is the input feature vector.\n",
    "�\n",
    "b is the bias term.\n",
    "The predicted class label \n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  for an input vector \n",
    "�\n",
    "x is determined based on the sign of \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x). Specifically, if \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "≥\n",
    "0\n",
    "f(x)≥0, the predicted class label is \n",
    "+\n",
    "1\n",
    "+1; otherwise, the predicted class label is \n",
    "−\n",
    "1\n",
    "−1.\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "Answer--The objective function of a linear Support Vector Machine (SVM) is \n",
    "formulated to find the optimal hyperplane that separates the data into \n",
    "different classes while maximizing the margin between the classes. In a \n",
    "linear SVM, the objective function is designed to minimize the classification\n",
    "error and maximize the margin simultaneously.\n",
    "\n",
    "The objective function of a linear SVM can be expressed as a combination of two components:\n",
    "\n",
    "Margin Maximization: The SVM aims to maximize the margin, which is the distance\n",
    "between the hyperplane and the nearest data points (support vectors). Maximizing \n",
    "the margin helps improve the generalization ability of the classifier.\n",
    "\n",
    "Classification Error Minimization: The SVM also aims to minimize the classification error, \n",
    "ensuring that data points are correctly classified according to their respective classes.\n",
    "\n",
    "The objective function of a linear SVM is often defined as a convex optimization problem. \n",
    "Mathematically, it can be represented as:\n",
    "\n",
    "min\n",
    "⁡\n",
    "�\n",
    ",\n",
    "�\n",
    "1\n",
    "2\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "min \n",
    "w,b\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " \n",
    "\n",
    "subject to the constraint:\n",
    "\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    "y \n",
    "i\n",
    "​\n",
    " (w \n",
    "T\n",
    " x \n",
    "i\n",
    "​\n",
    " +b)≥1\n",
    "\n",
    "for \n",
    "�\n",
    "=\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "�\n",
    "i=1,2,...,n, where:\n",
    "\n",
    "�\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "�\n",
    "b is the bias term.\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "∥w∥ \n",
    "2\n",
    "  represents the squared Euclidean norm of the weight vector.\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  represents the class label of the \n",
    "�\n",
    "ith training example.\n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  represents the feature vector of the \n",
    "�\n",
    "ith training example.\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "Answer--The kernel trick is a fundamental concept in Support Vector Machine \n",
    "(SVM) algorithms, particularly in cases where the data is not linearly separable\n",
    "\n",
    "in its original feature space. It allows SVMs to implicitly map input data into\n",
    "higher-dimensional feature spaces without explicitly computing the transformed \n",
    "feature vectors. This transformation enables SVMs to find linear decision boundaries \n",
    "in higher-dimensional spaces, even though the classification problem might be\n",
    "non-linear in the original feature space.\n",
    "\n",
    "The kernel trick works by introducing a kernel function \n",
    "�\n",
    "K that computes the inner product between pairs of data points in the higher-dimensional \n",
    "space without explicitly computing the transformation. Mathematically, the kernel function \n",
    "�\n",
    "K is defined as:\n",
    "    \n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Answer--In Support Vector Machine (SVM), support vectors play a crucial role in \n",
    "defining the decision boundary or hyperplane that separates different classes in \n",
    "the feature space. Support vectors are the data points that lie closest to the\n",
    "decision boundary, and they effectively determine the position and orientation\n",
    "of the decision boundary.\n",
    "\n",
    "Here's how support vectors function in SVM with an example:\n",
    "\n",
    "Consider a binary classification problem where we want to distinguish between \n",
    "two classes, say Class A and Class B, in a two-dimensional feature space.\n",
    "We have the following data points:\n",
    "    Class A: {(-1, 1), (0, 0), (1, 1)}\n",
    "Class B: {(-1, -1), (0, -1), (1, -1)}\n",
    "In the feature space, the SVM algorithm seeks to find the optimal \n",
    "hyperplane that maximizes the margin between the two classes while \n",
    "minimizing classification errors. The hyperplane is defined by the\n",
    "support vectors, which are the data points closest to the decision boundary.\n",
    "\n",
    "In this example, the support vectors are the points closest to the\n",
    "decision boundary, which will be the points (-1, 1), (1, 1), (-1, -1), and (1, -1).\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "Answer--To illustrate the concepts of hyperplane, marginal plane, soft margin, \n",
    "and hard margin in SVM, let's consider a simple two-dimensional classification \n",
    "problem with two classes: Class 1 (blue circles) and Class 2 (red squares).\n",
    "\n",
    "Here are the definitions of each concept:\n",
    "\n",
    "Hyperplane: In SVM, a hyperplane is a decision boundary that separates data points\n",
    "of different classes. For a two-dimensional problem, the hyperplane is a line. \n",
    "In higher dimensions, it becomes a hyperplane. The goal of SVM is to find the \n",
    "optimal hyperplane that maximizes the margin between the classes.\n",
    "\n",
    "Marginal plane: The marginal plane is the plane parallel to the hyperplane and \n",
    "touching the support vectors. It defines the margins of the SVM classifier.\n",
    "\n",
    "Soft margin: In soft margin SVM, the classifier allows for some misclassification \n",
    "of training examples to achieve a wider margin and better generalization to unseen\n",
    "data. Soft margin SVM uses a penalty parameter (C) to control the trade-off between \n",
    "maximizing the margin and minimizing the classification error.\n",
    "\n",
    "Hard margin: In hard margin SVM, the classifier does not allow any misclassification\n",
    "of training examples. It seeks to find a hyperplane that perfectly separates the\n",
    "classes if such a hyperplane exists. Hard margin SVM can be sensitive to outliers and noisy data.\n",
    "\n",
    "Let's visualize these concepts using matplotlib in Python:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "y = [0] * 20 + [1] * 20\n",
    "\n",
    "# Fit the model\n",
    "clf_hard = svm.SVC(kernel='linear', C=1)  # Hard margin SVM\n",
    "clf_soft = svm.SVC(kernel='linear', C=0.1)  # Soft margin SVM\n",
    "clf_hard.fit(X, y)\n",
    "clf_soft.fit(X, y)\n",
    "\n",
    "# Get the separating hyperplane\n",
    "w_hard = clf_hard.coef_[0]\n",
    "w_soft = clf_soft.coef_[0]\n",
    "a_hard = -w_hard[0] / w_hard[1]\n",
    "a_soft = -w_soft[0] / w_soft[1]\n",
    "xx = np.linspace(-5, 5)\n",
    "yy_hard = a_hard * xx - (clf_hard.intercept_[0]) / w_hard[1]\n",
    "yy_soft = a_soft * xx - (clf_soft.intercept_[0]) / w_soft[1]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=30)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Plot the hyperplane, margins, and support vectors\n",
    "plt.plot(xx, yy_hard, 'k-', label='Hard margin')\n",
    "plt.plot(xx, yy_soft, 'k--', label='Soft margin')\n",
    "plt.plot(xx, yy_hard + 1 / np.sqrt(np.sum(clf_hard.coef_ ** 2)), 'k:')\n",
    "plt.plot(xx, yy_hard - 1 / np.sqrt(np.sum(clf_hard.coef_ ** 2)), 'k:')\n",
    "plt.plot(xx, yy_soft + 1 / np.sqrt(np.sum(clf_soft.coef_ ** 2)), 'k-.')\n",
    "plt.plot(xx, yy_soft - 1 / np.sqrt(np.sum(clf_soft.coef_ ** 2)), 'k-.')\n",
    "plt.scatter(clf_hard.support_vectors_[:, 0], clf_hard.support_vectors_[:, 1],\n",
    "            s=100, facecolors='none', edgecolors='k', label='Support vectors')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "Answer--from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train a linear SVM classifier\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 5: Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 6: Plot the decision boundaries using two features\n",
    "# We'll choose the first two features for visualization\n",
    "X_subset = X[:, :2]\n",
    "X_train_subset = X_train[:, :2]\n",
    "X_test_subset = X_test[:, :2]\n",
    "\n",
    "# Plotting the decision boundaries\n",
    "x_min, x_max = X_subset[:, 0].min() - 1, X_subset[:, 0].max() + 1\n",
    "y_min, y_max = X_subset[:, 1].min() - 1, X_subset[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X_train_subset[:, 0], X_train_subset[:, 1], c=y_train, s=20, edgecolors='k', label='Training set')\n",
    "plt.scatter(X_test_subset[:, 0], X_test_subset[:, 1], c=y_test, s=100, marker='x', edgecolors='k', label='Testing set')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundaries of Linear SVM')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Experiment with different values of the regularization parameter C\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "for C in C_values:\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
